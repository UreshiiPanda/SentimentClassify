{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19ad6880-6ee3-449b-a89f-d4bf9753e016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Perceptron Dev Error 26.4%, |w|=16744, time: 5.7 secs\n",
      "% Pos on Test data: 41.8%\n",
      "Avg Perceptron Dev Error: 25.4%, |w|: 15806, time: 5.8 secs\n",
      "% Pos on Test data: 41.8%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %load train.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "from __future__ import division # no need for python3, but just in case used w/ python2\n",
    "\n",
    "import sys\n",
    "import time\n",
    "from svector import svector\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.linear_model import Perceptron\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### setup data and process it\n",
    "\n",
    "\n",
    "#trainfile = sys.argv[1]\n",
    "#devfile = sys.argv[2]\n",
    "#testfile = sys.argv[3]\n",
    "\n",
    "trainfile = \"train.txt\"\n",
    "devfile = \"dev.txt\"\n",
    "testfile = \"test.txt\"\n",
    "\n",
    "\n",
    "# load the embeddings\n",
    "wv = KeyedVectors.load('embs_train.kv')\n",
    "\n",
    "def parse_from(textfile):\n",
    "    for line in open(textfile):\n",
    "        label, words = line.strip().split(\"\\t\")\n",
    "        yield (1 if label==\"+\" else -1, words)\n",
    "\n",
    "\n",
    "def preprocess(file):\n",
    "    x_out = []\n",
    "    y_out = []\n",
    "\n",
    "    for i, (label, words) in enumerate(parse_from(file), 1):\n",
    "        x_out.append(words)\n",
    "        y_out.append(label)\n",
    "\n",
    "    return (x_out, y_out)\n",
    "\n",
    "\n",
    "def wv_process(file):\n",
    "    x_out = []\n",
    "    y_out = []\n",
    "\n",
    "    for i, (label, words) in enumerate(parse_from(file), 1):\n",
    "        # get word embeddings for this sentence\n",
    "        words = words.lower().split()\n",
    "        word_embeddings = [wv[word] for word in words if word in wv]\n",
    "        # now we have a list of lists, where each inner list is the wv of that word \n",
    "        if len(word_embeddings) > 0:\n",
    "            # take the mean of the wv's for this sentence\n",
    "            sentence_embedding = np.mean(word_embeddings, axis=0)\n",
    "            x_out.append(sentence_embedding.tolist())\n",
    "        else:\n",
    "            # if none of the words were in wv, generate a zero-vector\n",
    "            empty_sentence = np.zeros(wv.vector_size)\n",
    "            x_out.append(empty_sentence.tolist())\n",
    "        y_out.append(label)\n",
    "\n",
    "    return (x_out, y_out)\n",
    "\n",
    "\n",
    "## pre-process the data for word embeddings for dense vectors\n",
    "x_train, y_train = wv_process(trainfile)\n",
    "x_dev, y_dev = wv_process(devfile)\n",
    "x_test, y_test = wv_process(testfile)\n",
    "\n",
    "\n",
    "## scale the word-embeddings data\n",
    "# scaler = MinMaxScaler()\n",
    "# x_train = scaler.fit_transform(x_train)\n",
    "# x_dev = scaler.transform(x_dev)\n",
    "# x_test = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####   Perceptron\n",
    "\n",
    "\n",
    "def read_from(textfile):\n",
    "    for line in open(textfile):\n",
    "        label, words = line.strip().split(\"\\t\")\n",
    "        yield (1 if label==\"+\" else -1, words.split())\n",
    "\n",
    "\n",
    "def make_vector(words):\n",
    "    v = svector()\n",
    "    v['im_bias'] = 1.0\n",
    "    #v['im_bias'] = np.ones(wv.vector_size)     # add a bias feature, set it to 1, we now have d+1 dim\n",
    "    word_embeddings = [wv[word] for word in words if word in wv]\n",
    "    sentence_embedding = np.mean(word_embeddings, axis=0)\n",
    "    #v[\" \".join(words)] = sentence_embedding\n",
    "    for word in words:\n",
    "        v[word] = np.linalg.norm(sentence_embedding)\n",
    "        #v[word] = sentence_embedding\n",
    "    return v\n",
    "\n",
    "\n",
    "def test(devfile, model, dev_errs):\n",
    "    tot, err = 0, 0\n",
    "    for i, (label, words) in enumerate(read_from(devfile), 1): # note 1...|D|\n",
    "        sent = make_vector(words)\n",
    "        err += label * (model.dot(sent)) <= 0\n",
    "        # get max/min errors on dev\n",
    "        dev_errs.append((label, label * (model.dot(make_vector(words))), words))\n",
    "    return err/i  # i is |D| now\n",
    "\n",
    "\n",
    "\n",
    "def train(trainfile, devfile, epochs=5):\n",
    "    t = time.time()\n",
    "    best_err = 1.\n",
    "    model = svector()\n",
    "    model['im_bias'] = 0.0    \n",
    "    #model['im_bias'] = np.zeros(wv.vector_size)   \n",
    "    for it in range(1, epochs+1):\n",
    "        updates = 0\n",
    "        for i, (label, words) in enumerate(read_from(trainfile), 1): # label is +1 or -1\n",
    "            sent = make_vector(words)\n",
    "            if label * (model.dot(sent)) <= 0:\n",
    "                updates += 1\n",
    "                model += label * sent \n",
    "        dev_errs = []\n",
    "        dev_err = test(devfile, model, dev_errs)\n",
    "        best_err = min(best_err, dev_err)\n",
    "        #print(\"epoch %d, update %.1f%%, dev %.1f%%\" % (it, updates / i * 100, dev_err * 100))\n",
    "    print(\"Naive Perceptron Dev Error %.1f%%, |w|=%d, time: %.1f secs\" % (best_err * 100, len(model), time.time() - t))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def train_avg(trainfile, devfile, epochs=5):\n",
    "    t = time.time()\n",
    "    best_err = 1.\n",
    "    model = svector()\n",
    "    model['im_bias'] = 0.0\n",
    "    #model['im_bias'] = np.zeros(wv.vector_size)    \n",
    "    w_aux = svector()\n",
    "    count = 0\n",
    "    for it in range(1, epochs+1):\n",
    "        updates = 0\n",
    "        for i, (label, words) in enumerate(read_from(trainfile), 1): # label is +1 or -1\n",
    "            sent = make_vector(words)\n",
    "            if label * (model.dot(sent)) <= 0:  # model made a mistake\n",
    "                updates += 1\n",
    "                model += label * sent\n",
    "                w_aux += count * label * sent\n",
    "            count += 1 \n",
    "        dev_errs = []\n",
    "        dev_err = test(devfile, (count * model) - w_aux, dev_errs)\n",
    "        best_err = min(best_err, dev_err)\n",
    "\n",
    "\n",
    "\n",
    "        #print(\"epoch %d, update %.1f%%, dev %.1f%%\" % (it, updates / i * 100, dev_err * 100))\n",
    "    print(\"Avg Perceptron Dev Error: %.1f%%, |w|: %d, time: %.1f secs\" % (best_err * 100, len(model), time.time() - t))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# predict on test data\n",
    "def predict(testfile, model): \n",
    "    y_test_pred = []\n",
    "    t = time.time()\n",
    "    for i, (label, words) in enumerate(read_from(testfile), 1):\n",
    "        pred = (model.dot(make_vector(words)))\n",
    "        y_test_pred.append(1 if pred > 0 else -1)\n",
    "    y_test_tot = sum( [1 for label in y_test_pred if label == 1] )\n",
    "    print(f\"% Pos on Test data: {100*y_test_tot/1000:.1f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = train(trainfile, devfile, 10)\n",
    "predict(testfile, model)\n",
    "model_avg = train_avg(trainfile, devfile, 10)\n",
    "predict(testfile, model_avg)\n",
    "print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754d079b-9abb-4289-90ae-b0e4a1468363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac06bb6-9da0-4695-bd55-f3686be29593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e01d7c9-8242-4947-9b4f-4617e46cc489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b648fa2-bdd1-4b73-9258-c4d09e033171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d260cbe-cc07-4177-89a8-25101a5bdc39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a9e2a8-cdc0-4b94-a9cb-684eda6b917c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
